{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import sys\n",
    "from sklearn import linear_model\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(x,y,alpha,no_of_HL,no_of_units_HL):\n",
    "    total_no_of_layers = no_of_HL + 2\n",
    "    all_activation = {}\n",
    "    \n",
    "    cost_series = []\n",
    "    activation = x  \n",
    "    bais = np.array([1])      #creating the bais value\n",
    "    activation = np.append(bais,activation)\n",
    "    activation = (activation[np.newaxis]).T \n",
    "    all_activation[1] = activation\n",
    "    all_theta = {}\n",
    "    delta = {}\n",
    "    BIG_DELTA = {}\n",
    "    theta_history = []\n",
    "    \n",
    "    for k in range(1,total_no_of_layers):\n",
    "         #calculating the size theta_matrix\n",
    "        sj = activation.size-1\n",
    "        sj_1 = no_of_units_HL[k-1]\n",
    "        theta_size = sj_1 * (sj+1)\n",
    "        theta = np.random.uniform(0,1,theta_size)\n",
    "        theta = theta.reshape(sj_1,(sj+1))\n",
    "        all_theta[k] = theta\n",
    "        \n",
    "    theta_history.append(all_theta.copy())\n",
    "    \n",
    "    for a in range(1,400):\n",
    "        #Forward Propogation\n",
    "        for k in range(1,total_no_of_layers):\n",
    "            z = np.matmul(all_theta[k],all_activation[k])  #z = h_theta_of_x i.e. theta*x\n",
    "            activation = sigmoid(z)\n",
    "            bais = np.array([1])      #creating the bais value\n",
    "            activation = np.append(bais,activation)\n",
    "            activation = (activation[np.newaxis]).T       #converting activation into column matrix\n",
    "            all_activation[k+1] = activation\n",
    "\n",
    "        #back propogation\n",
    "\n",
    "        #calculating delta for last layer\n",
    "        delta[total_no_of_layers] = all_activation[total_no_of_layers][1:3,:] - y.T\n",
    "\n",
    "        #calculating delta for other layers (lowest delta possible is 2)\n",
    "        for l in range(total_no_of_layers-1, 1, -1):\n",
    "            delta[l] = np.multiply(np.matmul(all_theta[l].T,delta[l+1]),(all_activation[l]*(1-all_activation[l])))\n",
    "            delta[l] = delta[l][1:3,:]\n",
    "\n",
    "         #cost in this iteration\n",
    "        cost = np.sum(delta[total_no_of_layers]**2)/2\n",
    "        cost_series.append(cost)\n",
    "\n",
    "        #calculating BIG_delta - theta gradient  (it represents the direction of slope)\n",
    "        for l in range(total_no_of_layers-1, 0, -1):\n",
    "            tri_delta = np.matmul(delta[l+1],all_activation[l].T)\n",
    "            BIG_DELTA[l] = tri_delta\n",
    "            \n",
    "        for l in range(total_no_of_layers-1, 0, -1):\n",
    "            all_theta[l] = all_theta[l] - (alpha * BIG_DELTA[l])\n",
    "        \n",
    "        theta_history.append(all_theta.copy())\n",
    "        \n",
    "        print(all_activation[total_no_of_layers])\n",
    "    \n",
    "    return cost_series,theta_history\n",
    "    \n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris_raw.data\n",
    "y = iris_raw.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,3):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artificial_neural_network(x,y,alpha,no_of_HL,no_of_units_HL):\n",
    "    total_no_of_layers = no_of_HL + 2\n",
    "    all_activation = {}\n",
    "    BIG_DELTA = init()\n",
    "    cost_series = []\n",
    "    all_theta = {}\n",
    "    delta = {}\n",
    "    j_local = 0\n",
    "    theta_gradient = {}\n",
    "    \n",
    "    #initialising theta\n",
    "    all_theta = initialise_theta(x,no_of_units_HL,total_no_of_layers)\n",
    "    \n",
    "    \n",
    "    for skip in range(0,100):\n",
    "        x_test = x[skip]\n",
    "        x_train = np.delete(x, (skip), axis=0)\n",
    "        y_test = y[skip]\n",
    "        y_train = np.delete(y, (skip), axis=0)\n",
    "        for iteration in range(0,100):    \n",
    "            # initialising big delta matrix\n",
    "            BIG_DELTA = init()\n",
    "        \n",
    "            for itr in range(0,99):\n",
    "                #forward propogate\n",
    "                all_activation = calculate_all_activation(x_train[itr],all_theta,3)\n",
    "\n",
    "                #calculate error in each layer\n",
    "                delta = calcualte_delta(3,all_activation,y_train[itr],all_theta)\n",
    "\n",
    "                #Cost function\n",
    "                a3 = np.asscalar((all_activation[total_no_of_layers])[1,:])\n",
    "                j_local = j_local + (y_train[itr]*math.log10(a3) + ((1-y_train[itr])*math.log10(1-a3)))\n",
    "\n",
    "                #calculating BIG_delta -> theta gradient  (it represents the direction of slope)\n",
    "                for l in range(total_no_of_layers-1, 0, -1):\n",
    "                    tri_delta = np.matmul(delta[l+1],all_activation[l].T)\n",
    "                    BIG_DELTA[l] = BIG_DELTA[l] + tri_delta\n",
    "            \n",
    "            for l in range(total_no_of_layers-1, 0, -1):\n",
    "                theta_gradient[l] = BIG_DELTA[l]/99\n",
    "            \n",
    "            #Computing theta by subtracting theta gradient\n",
    "            for l in range(total_no_of_layers-1, 0, -1):\n",
    "                all_theta[l] = all_theta[l] - (alpha * theta_gradient[l])\n",
    "\n",
    "            #appending the cost to cost_series\n",
    "            cost_series.append(-(j_local/99))\n",
    "            j_local = 0\n",
    "        \n",
    "    op = []\n",
    "    for sample in range(0,100):\n",
    "        all_activation = calculate_all_activation(x[sample],all_theta,3)\n",
    "        if(all_activation[3][1,:]>=0.5):\n",
    "            op.append(1)\n",
    "        else:\n",
    "            op.append(0)\n",
    "    return cost_series,op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    BIG_DELTA = {}\n",
    "    BIG_DELTA[1] = np.zeros(6).reshape(2,3)\n",
    "    BIG_DELTA[2] = np.zeros(3).reshape(1,3)\n",
    "    return BIG_DELTA\n",
    "\n",
    "def calculate_all_activation(x,all_theta,total_no_of_layers):\n",
    "    all_activation = {}\n",
    "    activation = x  \n",
    "    bais = np.array([1])      #creating the bais value\n",
    "    activation = np.append(bais,activation)\n",
    "    activation = (activation[np.newaxis]).T \n",
    "    all_activation[1] = activation\n",
    "    for k in range(1,total_no_of_layers):\n",
    "        z = np.matmul(all_theta[k],all_activation[k])  #z = h_theta_of_x i.e. theta*x\n",
    "        activation = sigmoid(z)\n",
    "        bais = np.array([1])      #creating the bais value\n",
    "        activation = np.append(bais,activation)\n",
    "        activation = (activation[np.newaxis]).T       #converting activation into column matrix\n",
    "        all_activation[k+1] = activation\n",
    "    return all_activation\n",
    "\n",
    "def calcualte_delta(total_no_of_layers,all_activation,y,all_theta):\n",
    "    delta = {}\n",
    "    #calculating delta for last layer\n",
    "    delta[total_no_of_layers] = all_activation[total_no_of_layers][1,:] - y.T\n",
    "\n",
    "    #calculating delta for other layers (lowest delta possible is 2)\n",
    "    for l in range(total_no_of_layers-1, 1, -1):\n",
    "        temp = np.array(np.matmul(all_theta[l].T,delta[l+1]))\n",
    "        temp = (temp[np.newaxis]).T \n",
    "        delta[l] = np.multiply(temp,(all_activation[l]*(1-all_activation[l])))\n",
    "        delta[l] = delta[l][1:3,:]\n",
    "    return delta\n",
    "\n",
    "def initialise_theta(x,no_of_units_HL,total_no_of_layers):\n",
    "    all_theta = {}\n",
    "    activation = x[1]  \n",
    "    bais = np.array([1])      #creating the bais value\n",
    "    activation = np.append(bais,activation)\n",
    "    activation = (activation[np.newaxis]).T \n",
    "    for k in range(1,total_no_of_layers):\n",
    "         #calculating the size theta_matrix\n",
    "        sj = activation.size-1\n",
    "        sj_1 = no_of_units_HL[k-1]\n",
    "        theta_size = sj_1 * (sj+1)\n",
    "        np.random.seed(25)\n",
    "        theta = np.random.uniform(0,1,theta_size)\n",
    "        theta = theta.reshape(sj_1,(sj+1))\n",
    "        all_theta[k] = theta\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris_raw = datasets.load_iris()\n",
    "x = iris_raw.data[:,-2:][50:]\n",
    "y = iris_raw.target[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y==1] = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_units_HL = np.array([2,1])\n",
    "cost,op = artificial_neural_network(x,y,0.1,1,no_of_units_HL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a1a5f49e8>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEjCAYAAADdZh27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a53dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Cost v/s iteration')\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.scatter(np.arange(0,10000),cost, c= 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50242791608459803"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost[10000-1]\n",
    "cost[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array((0.05,0.1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for skip in range(1,100):\n",
    "    print(skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array((0.05,0.1))\n",
    "x = np.array([x])\n",
    "x = x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
